This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.github/
  workflows/
    release.yml
examples/
  basic_usage.rs
  live_transcription.rs
ref_audio/
  README.md
src/
  lib.rs
  main.rs
.gitignore
Cargo.toml
dist-workspace.toml
LICENSE
README.md
test_deserialize.rs
websocket_example.html
WEBSOCKET.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(cargo build:*)",
      "Bash(rg:*)",
      "Bash(cargo check:*)"
    ],
    "deny": []
  }
}
</file>

<file path=".github/workflows/release.yml">
# This file was autogenerated by dist: https://github.com/astral-sh/cargo-dist
#
# Copyright 2022-2024, axodotdev
# Copyright 2025 Astral Software Inc.
# SPDX-License-Identifier: MIT or Apache-2.0
#
# CI that:
#
# * checks for a Git Tag that looks like a release
# * builds artifacts with dist (archives, installers, hashes)
# * uploads those artifacts to temporary workflow zip
# * on success, uploads the artifacts to a GitHub Release
#
# Note that the GitHub Release will be created with a generated
# title/body based on your changelogs.

name: Release
permissions:
  "contents": "write"

# This task will run whenever you push a git tag that looks like a version
# like "1.0.0", "v0.1.0-prerelease.1", "my-app/0.1.0", "releases/v1.0.0", etc.
# Various formats will be parsed into a VERSION and an optional PACKAGE_NAME, where
# PACKAGE_NAME must be the name of a Cargo package in your workspace, and VERSION
# must be a Cargo-style SemVer Version (must have at least major.minor.patch).
#
# If PACKAGE_NAME is specified, then the announcement will be for that
# package (erroring out if it doesn't have the given version or isn't dist-able).
#
# If PACKAGE_NAME isn't specified, then the announcement will be for all
# (dist-able) packages in the workspace with that version (this mode is
# intended for workspaces with only one dist-able package, or with all dist-able
# packages versioned/released in lockstep).
#
# If you push multiple tags at once, separate instances of this workflow will
# spin up, creating an independent announcement for each one. However, GitHub
# will hard limit this to 3 tags per commit, as it will assume more tags is a
# mistake.
#
# If there's a prerelease-style suffix to the version, then the release(s)
# will be marked as a prerelease.
on:
  pull_request:
  push:
    tags:
      - '**[0-9]+.[0-9]+.[0-9]+*'

jobs:
  # Run 'dist plan' (or host) to determine what tasks we need to do
  plan:
    runs-on: "ubuntu-22.04"
    outputs:
      val: ${{ steps.plan.outputs.manifest }}
      tag: ${{ !github.event.pull_request && github.ref_name || '' }}
      tag-flag: ${{ !github.event.pull_request && format('--tag={0}', github.ref_name) || '' }}
      publishing: ${{ !github.event.pull_request }}
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          submodules: recursive
      - name: Install dist
        # we specify bash to get pipefail; it guards against the `curl` command
        # failing. otherwise `sh` won't catch that `curl` returned non-0
        shell: bash
        run: "curl --proto '=https' --tlsv1.2 -LsSf https://github.com/astral-sh/cargo-dist/releases/download/v0.28.7-prerelease.1/cargo-dist-installer.sh | sh"
      - name: Cache dist
        uses: actions/upload-artifact@v4
        with:
          name: cargo-dist-cache
          path: ~/.cargo/bin/dist
      # sure would be cool if github gave us proper conditionals...
      # so here's a doubly-nested ternary-via-truthiness to try to provide the best possible
      # functionality based on whether this is a pull_request, and whether it's from a fork.
      # (PRs run on the *source* but secrets are usually on the *target* -- that's *good*
      # but also really annoying to build CI around when it needs secrets to work right.)
      - id: plan
        run: |
          dist ${{ (!github.event.pull_request && format('host --steps=create --tag={0}', github.ref_name)) || 'plan' }} --output-format=json > plan-dist-manifest.json
          echo "dist ran successfully"
          cat plan-dist-manifest.json
          echo "manifest=$(jq -c "." plan-dist-manifest.json)" >> "$GITHUB_OUTPUT"
      - name: "Upload dist-manifest.json"
        uses: actions/upload-artifact@v4
        with:
          name: artifacts-plan-dist-manifest
          path: plan-dist-manifest.json

  # Build and packages all the platform-specific things
  build-local-artifacts:
    name: build-local-artifacts (${{ join(matrix.targets, ', ') }})
    # Let the initial task tell us to not run (currently very blunt)
    needs:
      - plan
    if: ${{ fromJson(needs.plan.outputs.val).ci.github.artifacts_matrix.include != null && (needs.plan.outputs.publishing == 'true' || fromJson(needs.plan.outputs.val).ci.github.pr_run_mode == 'upload') }}
    strategy:
      fail-fast: false
      # Target platforms/runners are computed by dist in create-release.
      # Each member of the matrix has the following arguments:
      #
      # - runner: the github runner
      # - dist-args: cli flags to pass to dist
      # - install-dist: expression to run to install dist on the runner
      #
      # Typically there will be:
      # - 1 "global" task that builds universal installers
      # - N "local" tasks that build each platform's binaries and platform-specific installers
      matrix: ${{ fromJson(needs.plan.outputs.val).ci.github.artifacts_matrix }}
    runs-on: ${{ matrix.runner }}
    container: ${{ matrix.container && matrix.container.image || null }}
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      BUILD_MANIFEST_NAME: target/distrib/${{ join(matrix.targets, '-') }}-dist-manifest.json
    steps:
      - name: enable windows longpaths
        run: |
          git config --global core.longpaths true
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          submodules: recursive
      - name: Install Rust non-interactively if not already installed
        if: ${{ matrix.container }}
        run: |
          if ! command -v cargo > /dev/null 2>&1; then
            curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
            echo "$HOME/.cargo/bin" >> $GITHUB_PATH
          fi
      - name: Install dist
        run: ${{ matrix.install_dist.run }}
      # Get the dist-manifest
      - name: Fetch local artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: artifacts-*
          path: target/distrib/
          merge-multiple: true
      - name: Install dependencies
        run: |
          ${{ matrix.packages_install }}
      - name: Build artifacts
        run: |
          # Actually do builds and make zips and whatnot
          dist build ${{ needs.plan.outputs.tag-flag }} --print=linkage --output-format=json ${{ matrix.dist_args }} > dist-manifest.json
          echo "dist ran successfully"
      - id: cargo-dist
        name: Post-build
        # We force bash here just because github makes it really hard to get values up
        # to "real" actions without writing to env-vars, and writing to env-vars has
        # inconsistent syntax between shell and powershell.
        shell: bash
        run: |
          # Parse out what we just built and upload it to scratch storage
          echo "paths<<EOF" >> "$GITHUB_OUTPUT"
          dist print-upload-files-from-manifest --manifest dist-manifest.json >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

          cp dist-manifest.json "$BUILD_MANIFEST_NAME"
      - name: "Upload artifacts"
        uses: actions/upload-artifact@v4
        with:
          name: artifacts-build-local-${{ join(matrix.targets, '_') }}
          path: |
            ${{ steps.cargo-dist.outputs.paths }}
            ${{ env.BUILD_MANIFEST_NAME }}

  # Build and package all the platform-agnostic(ish) things
  build-global-artifacts:
    needs:
      - plan
      - build-local-artifacts
    runs-on: "ubuntu-22.04"
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      BUILD_MANIFEST_NAME: target/distrib/global-dist-manifest.json
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          submodules: recursive
      - name: Install cached dist
        uses: actions/download-artifact@v4
        with:
          name: cargo-dist-cache
          path: ~/.cargo/bin/
      - run: chmod +x ~/.cargo/bin/dist
      # Get all the local artifacts for the global tasks to use (for e.g. checksums)
      - name: Fetch local artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: artifacts-*
          path: target/distrib/
          merge-multiple: true
      - id: cargo-dist
        shell: bash
        run: |
          dist build ${{ needs.plan.outputs.tag-flag }} --output-format=json "--artifacts=global" > dist-manifest.json
          echo "dist ran successfully"

          # Parse out what we just built and upload it to scratch storage
          echo "paths<<EOF" >> "$GITHUB_OUTPUT"
          jq --raw-output ".upload_files[]" dist-manifest.json >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

          cp dist-manifest.json "$BUILD_MANIFEST_NAME"
      - name: "Upload artifacts"
        uses: actions/upload-artifact@v4
        with:
          name: artifacts-build-global
          path: |
            ${{ steps.cargo-dist.outputs.paths }}
            ${{ env.BUILD_MANIFEST_NAME }}
  # Determines if we should publish/announce
  host:
    needs:
      - plan
      - build-local-artifacts
      - build-global-artifacts
    # Only run if we're "publishing", and only if local and global didn't fail (skipped is fine)
    if: ${{ always() && needs.plan.outputs.publishing == 'true' && (needs.build-global-artifacts.result == 'skipped' || needs.build-global-artifacts.result == 'success') && (needs.build-local-artifacts.result == 'skipped' || needs.build-local-artifacts.result == 'success') }}
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    runs-on: "ubuntu-22.04"
    outputs:
      val: ${{ steps.host.outputs.manifest }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          submodules: recursive
      - name: Install cached dist
        uses: actions/download-artifact@v4
        with:
          name: cargo-dist-cache
          path: ~/.cargo/bin/
      - run: chmod +x ~/.cargo/bin/dist
      # Fetch artifacts from scratch-storage
      - name: Fetch artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: artifacts-*
          path: target/distrib/
          merge-multiple: true
      - id: host
        shell: bash
        run: |
          dist host ${{ needs.plan.outputs.tag-flag }} --steps=upload --steps=release --output-format=json > dist-manifest.json
          echo "artifacts uploaded and released successfully"
          cat dist-manifest.json
          echo "manifest=$(jq -c "." dist-manifest.json)" >> "$GITHUB_OUTPUT"
      - name: "Upload dist-manifest.json"
        uses: actions/upload-artifact@v4
        with:
          # Overwrite the previous copy
          name: artifacts-dist-manifest
          path: dist-manifest.json
      # Create a GitHub Release while uploading all files to it
      - name: "Download GitHub Artifacts"
        uses: actions/download-artifact@v4
        with:
          pattern: artifacts-*
          path: artifacts
          merge-multiple: true
      - name: Cleanup
        run: |
          # Remove the granular manifests
          rm -f artifacts/*-dist-manifest.json
      - name: Create GitHub Release
        env:
          PRERELEASE_FLAG: "${{ fromJson(steps.host.outputs.manifest).announcement_is_prerelease && '--prerelease' || '' }}"
          ANNOUNCEMENT_TITLE: "${{ fromJson(steps.host.outputs.manifest).announcement_title }}"
          ANNOUNCEMENT_BODY: "${{ fromJson(steps.host.outputs.manifest).announcement_github_body }}"
          RELEASE_COMMIT: "${{ github.sha }}"
        run: |
          # Write and read notes from a file to avoid quoting breaking things
          echo "$ANNOUNCEMENT_BODY" > $RUNNER_TEMP/notes.txt

          gh release create "${{ needs.plan.outputs.tag }}" --target "$RELEASE_COMMIT" $PRERELEASE_FLAG --title "$ANNOUNCEMENT_TITLE" --notes-file "$RUNNER_TEMP/notes.txt" artifacts/*

  publish-homebrew-formula:
    needs:
      - plan
      - host
    runs-on: "ubuntu-22.04"
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      PLAN: ${{ needs.plan.outputs.val }}
      GITHUB_USER: "axo bot"
      GITHUB_EMAIL: "admin+bot@axo.dev"
    if: ${{ !fromJson(needs.plan.outputs.val).announcement_is_prerelease || fromJson(needs.plan.outputs.val).publish_prereleases }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          repository: "byteowlz/homebrew-tap"
          token: ${{ secrets.HOMEBREW_TAP_TOKEN }}
      # So we have access to the formula
      - name: Fetch homebrew formulae
        uses: actions/download-artifact@v4
        with:
          pattern: artifacts-*
          path: Formula/
          merge-multiple: true
      # This is extra complex because you can make your Formula name not match your app name
      # so we need to find releases with a *.rb file, and publish with that filename.
      - name: Commit formula files
        run: |
          git config --global user.name "${GITHUB_USER}"
          git config --global user.email "${GITHUB_EMAIL}"

          for release in $(echo "$PLAN" | jq --compact-output '.releases[] | select([.artifacts[] | endswith(".rb")] | any)'); do
            filename=$(echo "$release" | jq '.artifacts[] | select(endswith(".rb"))' --raw-output)
            name=$(echo "$filename" | sed "s/\.rb$//")
            version=$(echo "$release" | jq .app_version --raw-output)

            export PATH="/home/linuxbrew/.linuxbrew/bin:$PATH"
            brew update
            # We avoid reformatting user-provided data such as the app description and homepage.
            brew style --except-cops FormulaAudit/Homepage,FormulaAudit/Desc,FormulaAuditStrict --fix "Formula/${filename}" || true

            git add "Formula/${filename}"
            git commit -m "${name} ${version}"
          done
          git push

  announce:
    needs:
      - plan
      - host
      - publish-homebrew-formula
    # use "always() && ..." to allow us to wait for all publish jobs while
    # still allowing individual publish jobs to skip themselves (for prereleases).
    # "host" however must run to completion, no skipping allowed!
    if: ${{ always() && needs.host.result == 'success' && (needs.publish-homebrew-formula.result == 'skipped' || needs.publish-homebrew-formula.result == 'success') }}
    runs-on: "ubuntu-22.04"
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          submodules: recursive
</file>

<file path="examples/basic_usage.rs">
// Basic usage example for the eaRS library

use anyhow::Result;
use ears::{Model, TranscriptionOptions};

fn main() -> Result<()> {
    // Create transcription options
    let options = TranscriptionOptions {
        timestamps: true,
        vad: false,
        save_audio: Some("recorded_audio.wav".to_string()),
    };

    // Load the model
    let mut model = Model::load_from_hf("kyutai/stt-1b-en_fr-candle", false, options)?;

    // Transcribe an audio file
    let result = model.transcribe_file("path/to/your/audio.wav", Some("saved_audio.wav"))?;

    // Print the transcription
    println!("Transcription: {}", result.text);

    // Print word-level timestamps
    for word in result.words {
        if let Some(end_time) = word.end_time {
            println!("[{:5.2}-{:5.2}] {}", word.start_time, end_time, word.word);
        } else {
            println!("[{:5.2}-     ] {}", word.start_time, word.word);
        }
    }

    Ok(())
}
</file>

<file path="examples/live_transcription.rs">
// Live transcription example for the eaRS library

use anyhow::Result;
use crossbeam_channel::unbounded;
use ears::{Model, TranscriptionOptions, audio};
use std::thread;

fn main() -> Result<()> {
    // Create transcription options
    let options = TranscriptionOptions {
        timestamps: false,
        vad: true,
        save_audio: Some("live_recording.wav".to_string()),
    };

    // Load the model
    let mut model = Model::load_from_hf("kyutai/stt-1b-en_fr-candle", false, options)?;

    // Set up audio capture
    let (audio_tx, audio_rx) = unbounded();

    // Start audio capture in a separate thread
    let audio_handle = thread::spawn(move || {
        if let Err(e) = audio::start_audio_capture(audio_tx) {
            eprintln!("Audio capture error: {}", e);
        }
    });

    println!("Starting live transcription. Press Ctrl+C to stop.");
    println!("Speak into your microphone...");

    // Run live transcription
    let result = model.transcribe_live(audio_rx, Some("live_session.wav"))?;

    println!("\nFinal transcription: {}", result.text);

    audio_handle.join().unwrap();
    Ok(())
}
</file>

<file path="ref_audio/README.md">
This directory should contain short MP3 snippets used to prime the transcription model for other languages.
Provide files named `esp.mp3`, `ger.mp3`, or `jap.mp3` with a few seconds of speech in Spanish, German or Japanese respectively.
</file>

<file path=".gitignore">
target/
/.opencode
.DS_Store
/build
.env
.env.*
!.env.example
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Tommy Falkowski

Permission is hereby granted, free of charge, to any person obtaining a copy  
of this software and associated documentation files (the "Software"), to deal  
in the Software without restriction, including without limitation the rights  
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell  
copies of the Software, and to permit persons to whom the Software is  
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all  
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR  
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,  
FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE  
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER  
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM,  
OUT OF, OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE  
SOFTWARE.
</file>

<file path="test_deserialize.rs">
fn main() { let cmd = serde_json::from_str::<eaRS::WebSocketCommand>(r#"{"Restart":{}}"#).unwrap(); println\!("{:?}", cmd); }
</file>

<file path="WEBSOCKET.md">
# WebSocket Implementation Guide

This guide explains how to integrate eaRS's real-time speech-to-text WebSocket implementation into your applications.

## Overview

eaRS provides a WebSocket server that streams real-time transcription results, allowing you to build applications that need live speech recognition capabilities. The implementation supports multiple concurrent connections, real-time word streaming, and session control commands.

**Important**: The WebSocket server starts in **paused mode** by default. You must send a `Resume` command to begin transcription.

## Quick Start

### Starting the WebSocket Server

```bash
# Basic WebSocket server on port 8080
ears --live --ws 8080

# With additional features
ears --live --ws 8080 --timestamps --vad --vad-timeout 3.0
```

### Connecting from JavaScript

```javascript
const ws = new WebSocket('ws://localhost:8080/');

ws.onopen = () => {
    console.log('Connected to eaRS WebSocket');
    
    // Resume transcription to start receiving messages
    ws.send(JSON.stringify({ "Resume": {} }));
};

ws.onmessage = (event) => {
    const message = JSON.parse(event.data);
    handleTranscriptionMessage(message);
};

ws.onclose = () => {
    console.log('WebSocket connection closed');
};
```

## Message Types

### Incoming Messages (Server → Client)

#### 1. Word Messages
Sent for each transcribed word in real-time:

```json
{
    "Word": {
        "word": "hello",
        "start_time": 1.234,
        "end_time": 1.567
    }
}
```

#### 2. Pause Messages
Sent when voice activity detection detects silence:

```json
{
    "Pause": {
        "timestamp": 2.345
    }
}
```

#### 3. Final Messages
Sent at the end of utterances with complete text and word timings:

```json
{
    "Final": {
        "text": "hello world how are you",
        "words": [
            {
                "word": "hello",
                "start_time": 1.234,
                "end_time": 1.567
            },
            {
                "word": "world",
                "start_time": 1.678,
                "end_time": 1.890
            }
        ]
    }
}
```

### Outgoing Commands (Client → Server)

#### 1. Restart Command
Starts a new transcription session:

```json
{
    "Restart": {}
}
```

#### 2. Pause Command
Temporarily pauses transcription:

```json
{
    "Pause": {}
}
```

#### 3. Resume Command
Resumes paused transcription (required to start transcription as server starts in paused mode):

```json
{
    "Resume": {}
}
```

## Implementation Examples

### JavaScript/TypeScript Client

```javascript
class EarsWebSocketClient {
    constructor(port = 8080) {
        this.ws = new WebSocket(`ws://localhost:${port}/`);
        this.setupEventHandlers();
    }

    setupEventHandlers() {
        this.ws.onopen = () => {
            console.log('Connected to eaRS WebSocket');
            // Start transcription by sending Resume command
            this.resume();
        };
        
        this.ws.onmessage = (event) => {
            const message = JSON.parse(event.data);
            
            if (message.Word) {
                this.onWord(message.Word);
            } else if (message.Pause) {
                this.onPause(message.Pause);
            } else if (message.Final) {
                this.onFinal(message.Final);
            }
        };
    }

    onWord(wordData) {
        console.log(`Word: ${wordData.word} (${wordData.start_time}s)`);
        // Update your UI with the new word
    }

    onPause(pauseData) {
        console.log(`Paused at ${pauseData.timestamp}s`);
        // Handle pause in your UI
    }

    onFinal(finalData) {
        console.log(`Final: ${finalData.text}`);
        // Process complete utterance
    }

    restart() {
        this.ws.send(JSON.stringify({ "Restart": {} }));
    }

    pause() {
        this.ws.send(JSON.stringify({ "Pause": {} }));
    }

    resume() {
        this.ws.send(JSON.stringify({ "Resume": {} }));
    }
}

// Usage
const client = new EarsWebSocketClient(8080);
```

### Python Client

```python
import json
import asyncio
import websockets

class EarsWebSocketClient:
    def __init__(self, port=8080):
        self.uri = f"ws://localhost:{port}/"
        
    async def connect_and_listen(self):
        async with websockets.connect(self.uri) as websocket:
            # Start transcription by sending Resume command
            await websocket.send(json.dumps({"Resume": {}}))
            
            async for message in websocket:
                data = json.loads(message)
                await self.handle_message(data)
    
    async def handle_message(self, message):
        if "Word" in message:
            word_data = message["Word"]
            print(f"Word: {word_data['word']} ({word_data['start_time']}s)")
        elif "Pause" in message:
            pause_data = message["Pause"]
            print(f"Paused at {pause_data['timestamp']}s")
        elif "Final" in message:
            final_data = message["Final"]
            print(f"Final: {final_data['text']}")
    
    async def send_command(self, websocket, command):
        await websocket.send(json.dumps(command))

# Usage
async def main():
    client = EarsWebSocketClient(8080)
    await client.connect_and_listen()

asyncio.run(main())
```

### React Component Example

```tsx
import React, { useState, useEffect, useRef } from 'react';

interface WordData {
    word: string;
    start_time: number;
    end_time?: number;
}

interface TranscriptionMessage {
    Word?: WordData;
    Pause?: { timestamp: number };
    Final?: { text: string; words: WordData[] };
}

export const LiveTranscription: React.FC = () => {
    const [currentText, setCurrentText] = useState('');
    const [finalTexts, setFinalTexts] = useState<string[]>([]);
    const [isConnected, setIsConnected] = useState(false);
    const wsRef = useRef<WebSocket | null>(null);

    useEffect(() => {
        const ws = new WebSocket('ws://localhost:8080/');
        wsRef.current = ws;

        ws.onopen = () => {
            setIsConnected(true);
            // Start transcription by sending Resume command
            ws.send(JSON.stringify({ Resume: {} }));
        };
        ws.onclose = () => setIsConnected(false);
        
        ws.onmessage = (event) => {
            const message: TranscriptionMessage = JSON.parse(event.data);
            
            if (message.Word) {
                setCurrentText(prev => prev + message.Word!.word + ' ');
            } else if (message.Pause) {
                // Handle pause if needed
            } else if (message.Final) {
                setFinalTexts(prev => [...prev, message.Final!.text]);
                setCurrentText('');
            }
        };

        return () => ws.close();
    }, []);

    const sendCommand = (command: object) => {
        if (wsRef.current?.readyState === WebSocket.OPEN) {
            wsRef.current.send(JSON.stringify(command));
        }
    };

    return (
        <div>
            <div>Status: {isConnected ? 'Connected' : 'Disconnected'}</div>
            
            <div>
                <button onClick={() => sendCommand({ Restart: {} })}>Restart</button>
                <button onClick={() => sendCommand({ Pause: {} })}>Pause</button>
                <button onClick={() => sendCommand({ Resume: {} })}>Resume</button>
            </div>
            
            <div>
                <h3>Live Transcription:</h3>
                <p>{currentText}</p>
            </div>
            
            <div>
                <h3>Final Texts:</h3>
                {finalTexts.map((text, index) => (
                    <p key={index}>{text}</p>
                ))}
            </div>
        </div>
    );
};
```

## Server Configuration Options

When starting the eaRS WebSocket server, you can use these additional flags:

- `--timestamps`: Include word-level timestamps
- `--vad`: Enable Voice Activity Detection
- `--vad-timeout <seconds>`: Set VAD timeout (default: 2.0s)
- `--model <path>`: Specify custom model path
- `--device <device>`: Set device (auto, cpu, cuda, metal)

Example:
```bash
ears --live --ws 8080 --timestamps --vad --vad-timeout 3.0 --device cuda
```

## Error Handling

### Connection Management

```javascript
class RobustWebSocketClient {
    constructor(port, maxRetries = 5) {
        this.port = port;
        this.maxRetries = maxRetries;
        this.retryCount = 0;
        this.connect();
    }

    connect() {
        this.ws = new WebSocket(`ws://localhost:${this.port}/`);
        
        this.ws.onopen = () => {
            console.log('Connected');
            this.retryCount = 0;
        };
        
        this.ws.onclose = () => {
            if (this.retryCount < this.maxRetries) {
                console.log(`Reconnecting... (${this.retryCount + 1}/${this.maxRetries})`);
                setTimeout(() => {
                    this.retryCount++;
                    this.connect();
                }, 1000 * Math.pow(2, this.retryCount));
            }
        };
        
        this.ws.onerror = (error) => {
            console.error('WebSocket error:', error);
        };
    }
}
```

## Performance Considerations

1. **Message Frequency**: Word messages are sent in real-time, which can be high-frequency. Consider throttling UI updates if needed.

2. **Connection Limits**: The server supports multiple concurrent connections, but consider your system's limits.

3. **Buffer Management**: For long sessions, implement proper buffer management to prevent memory leaks.

4. **Network Resilience**: Implement reconnection logic for production applications.

## Testing

Use the included `websocket_example.html` file for testing:

```bash
# Start the server
ears --live --ws 8080

# Open websocket_example.html in your browser
# The example provides a complete testing interface
```

## Integration Patterns

### Real-time Captioning
Stream words to display live captions with timing information.

### Voice Commands
Use final text messages to implement voice command recognition.

### Transcription Logging
Capture and store complete transcriptions using final messages.

### Interactive Applications
Use pause/resume commands to control transcription flow based on application state.

## Troubleshooting

### Common Issues

1. **Connection Refused**: Ensure eaRS server is running with `--ws` flag
2. **No Audio**: Check microphone permissions and audio device settings
3. **High Latency**: Consider adjusting VAD timeout or using faster hardware
4. **Message Parsing Errors**: Ensure you're handling all message types properly

### Debug Mode

Run with verbose logging:
```bash
RUST_LOG=debug ears --live --ws 8080
```

For more information, see the main README.md file and the included `websocket_example.html` for a complete working example.
</file>

<file path="dist-workspace.toml">
[workspace]
members = ["cargo:."]

# Config for 'dist'
[dist]
# The preferred dist version to use in CI (Cargo.toml SemVer syntax)
cargo-dist-version = "0.28.7-prerelease.1"
# CI backends to support
ci = "github"
# The installers to generate for each app
installers = ["shell", "powershell", "npm", "homebrew"]
# A GitHub repo to push Homebrew formulas to
tap = "byteowlz/homebrew-tap"
# Target platforms to build apps for (Rust target-triple syntax)
targets = ["aarch64-apple-darwin", "aarch64-unknown-linux-gnu", "x86_64-apple-darwin", "x86_64-unknown-linux-gnu", "x86_64-pc-windows-msvc"]
# Path that installers should place binaries in
install-path = "CARGO_HOME"
# Publish jobs to run in CI
publish-jobs = ["homebrew"]
# Whether to install an updater program
install-updater = false

[workspace.metadata.dist.dependencies.apt]
libasound2-dev = "*"
pkg-config = "*"
libssl-dev = "*"
</file>

<file path="websocket_example.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>eaRS WebSocket Transcription Client</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        #messages {
            border: 1px solid #ccc;
            height: 400px;
            overflow-y: auto;
            padding: 10px;
            margin: 10px 0;
            background-color: #f9f9f9;
        }
        .word { color: #333; }
        .pause { color: #999; font-style: italic; }
        .final { color: #006600; font-weight: bold; }
        .error { color: #cc0000; }
        input, button {
            padding: 5px 10px;
            margin: 5px;
        }
    </style>
</head>
<body>
    <h1>eaRS WebSocket Transcription Client</h1>
    
    <div>
        <label>WebSocket URL: </label>
        <input type="text" id="wsUrl" value="ws://localhost:8080/" size="30">
        <button onclick="connect()">Connect</button>
        <button onclick="disconnect()">Disconnect</button>
        <button id="pauseBtn" onclick="togglePause()" disabled>Pause</button>
        <button id="restartBtn" onclick="restart()" disabled>Restart</button>
    </div>
    
    <div>Status: <span id="status">Disconnected</span></div>
    
    <div id="messages"></div>
    
    <div>
        <h3>Instructions:</h3>
        <ol>
            <li>Start eaRS with WebSocket: <code>ears --live --ws 8080</code></li>
            <li>Click "Connect" above (automatically resumes transcription)</li>
            <li>Start speaking - words will appear in real-time</li>
            <li>Use "Pause"/"Resume" button to control transcription</li>
            <li>Use "Restart" button to begin a new transcription session</li>
        </ol>
        <p><strong>Note:</strong> The WebSocket server starts in paused mode and automatically resumes when you connect.</p>
    </div>

    <script>
        let ws = null;
        let isPaused = false;
        const messagesDiv = document.getElementById('messages');
        const statusSpan = document.getElementById('status');
        const pauseBtn = document.getElementById('pauseBtn');
        const restartBtn = document.getElementById('restartBtn');

        function connect() {
            const url = document.getElementById('wsUrl').value;
            
            if (ws) {
                ws.close();
            }
            
            try {
                ws = new WebSocket(url);
                
                ws.onopen = function() {
                    statusSpan.textContent = 'Connected';
                    statusSpan.style.color = 'green';
                    pauseBtn.disabled = false;
                    restartBtn.disabled = false;
                    pauseBtn.textContent = 'Resume';
                    isPaused = true;
                    addMessage('Connected to eaRS WebSocket server (starting paused)', 'info');
                    // Server starts in paused mode, send Resume to begin transcription
                    ws.send(JSON.stringify("Resume"));
                    setTimeout(() => {
                        pauseBtn.textContent = 'Pause';
                        isPaused = false;
                        addMessage('Transcription started', 'info');
                    }, 100);
                };
                
                ws.onmessage = function(event) {
                    try {
                        const message = JSON.parse(event.data);
                        handleMessage(message);
                    } catch (e) {
                        addMessage('Error parsing message: ' + e.message, 'error');
                    }
                };
                
                ws.onclose = function() {
                    statusSpan.textContent = 'Disconnected';
                    statusSpan.style.color = 'red';
                    pauseBtn.disabled = true;
                    restartBtn.disabled = true;
                    pauseBtn.textContent = 'Pause';
                    isPaused = false;
                    addMessage('Disconnected from server', 'info');
                    ws = null;
                };
                
                ws.onerror = function(error) {
                    addMessage('WebSocket error: ' + error, 'error');
                };
                
            } catch (e) {
                addMessage('Connection error: ' + e.message, 'error');
            }
        }
        
        function disconnect() {
            if (ws) {
                ws.close();
                ws = null;
            }
            pauseBtn.disabled = true;
            restartBtn.disabled = true;
            pauseBtn.textContent = 'Pause';
            isPaused = false;
        }

        function togglePause() {
            if (!ws) return;
            if (isPaused) {
                ws.send(JSON.stringify("Resume"));
                pauseBtn.textContent = 'Pause';
                isPaused = false;
                addMessage('Transcription resumed', 'info');
            } else {
                ws.send(JSON.stringify("Pause"));
                pauseBtn.textContent = 'Resume';
                isPaused = true;
                addMessage('Transcription paused', 'info');
            }
        }

        function restart() {
            if (!ws) return;
            ws.send(JSON.stringify("Restart"));
            addMessage('Restart command sent - new transcription session started', 'info');
            // Clear previous messages
            messagesDiv.innerHTML = '';
            addMessage('Transcription session restarted', 'info');
        }
        
        function handleMessage(message) {
            if (message.Word) {
                const word = message.Word;
                const endTime = word.end_time ? ` (${word.start_time.toFixed(2)}-${word.end_time.toFixed(2)}s)` : ` (${word.start_time.toFixed(2)}s)`;
                addMessage(`Word: "${word.word}"${endTime}`, 'word');
            } else if (message.Pause) {
                const pause = message.Pause;
                addMessage(`Pause detected at ${new Date(pause.timestamp * 1000).toLocaleTimeString()}`, 'pause');
            } else if (message.Final) {
                const final = message.Final;
                addMessage('--- FINAL RESULT ---', 'final');
                addMessage(`Complete text: "${final.text}"`, 'final');
                addMessage(`Total words: ${final.words.length}`, 'final');
            } else {
                addMessage(`Unknown message format: ${JSON.stringify(message)}`, 'error');
            }
        }
        
        function addMessage(text, className) {
            const div = document.createElement('div');
            div.className = className;
            div.textContent = `[${new Date().toLocaleTimeString()}] ${text}`;
            messagesDiv.appendChild(div);
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
        }
    </script>
</body>
</html>
</file>

<file path="README.md">
# eaRS

A Rust-based speech-to-text transcription tool using Kyutai's STT models.

## Features

- Real-time transcription from microphone
- File-based audio transcription
- Word-level timestamps
- Voice activity detection (VAD)
- Audio device selection
- Audio recording to WAV files
- Multiple audio format support (WAV, MP3, OGG)
- WebSocket streaming for real-time applications
- Automatic termination after voice activity timeout
- WebSocket session restart capability

## Installation

```bash
cargo build --release
```

## Usage

### Live Transcription

```bash
# Use default microphone
./target/release/ears --live

# Select specific audio device
./target/release/ears --live --device 1

# With timestamps and VAD
./target/release/ears --live --timestamps --vad

# Save audio while transcribing
./target/release/ears --live --save-audio recording.wav

# Prime the model with reference audio in another language
./target/release/ears --live -l ger

# Automatically terminate after 5 seconds of no voice activity
./target/release/ears --live --vad-timeout 5.0
```

### WebSocket Streaming

```bash
# Start WebSocket server on port 8080
./target/release/ears --live --ws 8080

# With timestamps and VAD
./target/release/ears --live --ws 8080 --timestamps --vad

# With automatic timeout after 3 seconds of silence
./target/release/ears --live --ws 8080 --vad-timeout 3.0
```

### File Transcription

```bash
# Transcribe audio file
./target/release/ears audio.wav

# With timestamps
./target/release/ears audio.mp3 --timestamps
```

### Device Management

```bash
# List available audio devices
./target/release/ears --list-devices
```

## Options

- `--live` - Use live microphone input
- `--device <INDEX>` - Select audio input device by index
- `--timestamps` - Display word-level timestamps
- `--vad` - Show voice activity detection
- `--save-audio <FILE>` - Save audio to WAV file
- `--cpu` - Force CPU inference (disable GPU)
- `--hf-repo <REPO>` - Specify Hugging Face model repository
- `--list-devices` - List available audio devices
- `-l, --lang <LANG>` - Prime language using audio snippet (esp, ger, jap)
- `--ws <PORT>` - Start WebSocket server on specified port
- `--vad-timeout <SECONDS>` - Automatically terminate after no voice activity

## WebSocket API

When using the `--ws` option, eaRS starts a WebSocket server that streams real-time transcription results.

### Connection

Connect to `ws://localhost:<port>/` where `<port>` is specified via the `--ws` option.

### Message Types

#### Word Messages
Sent for each transcribed word as it's recognized:
```json
{
  "type": "word",
  "word": "hello",
  "start_time": 1.23,
  "end_time": 1.45
}
```

#### Pause Messages
Sent when voice activity detection detects a pause (requires `--vad` flag):
```json
{
  "type": "pause",
  "timestamp": 1234567890.123
}
```

#### Final Messages
Sent at the end of each transcription session:
```json
{
  "type": "final",
  "text": "complete transcribed text",
  "words": [
    {"word": "hello", "start_time": 1.23, "end_time": 1.45},
    {"word": "world", "start_time": 1.46, "end_time": null}
  ]
}
```

### Client Commands

#### Restart Transcription
Send from client to restart transcription after timeout or final message:
```json
{
  "type": "restart"
}
```

#### Pause/Resume Transcription
Toggle live inference without disconnecting:
```json
{ "type": "pause" }
{ "type": "resume" }
```

### Usage Pattern

1. Connect to WebSocket endpoint
2. Receive real-time word messages during transcription
3. Receive final message when session ends (timeout or silence)
4. Send restart command to begin new transcription session
5. Optionally send pause/resume commands to temporarily stop inference
6. Repeat as needed

## Model

Default model: `kyutai/stt-1b-en_fr-candle`

Supports English and French transcription with 24kHz audio processing. German also seems to work quite nicely.

## Requirements

- Rust 1.70+
- Audio input device for live transcription
- GPU support (CUDA/Metal) optional, falls back to CPU
</file>

<file path="Cargo.toml">
[package]
name = "eaRS"
version = "0.1.0"
edition = "2024"
description = "Easy automatic speech recognition library using Kyutai's STT models"
license = "MIT"
repository = "https://github.com/tommyfalkowski/eaRS"
authors = ["Tommy Falkowski"]
homepage = "https://byteowlz.com"

[lib]
name = "ears"
path = "src/lib.rs"

[[bin]]
name = "ears"
path = "src/main.rs"

[dependencies]
anyhow = "1.0"
atty = "0.2"
candle = { version = "0.9.1", package = "candle-core" }
candle-nn = "0.9.1"
clap = { version = "4.4.12", features = ["derive"] }
cpal = "0.15"
crossbeam-channel = "0.5"
hf-hub = "0.4.3"
kaudio = "0.2.1"
moshi = "0.6.1"
sentencepiece = "0.11.3"
serde = { version = "1.0.210", features = ["derive"] }
serde_json = "1.0.115"
tokio = { version = "1.0", features = ["full"] }
tokio-tungstenite = "0.20"
futures = "0.3"

[features]
default = []
cuda = ["candle/cuda", "candle-nn/cuda"]
cudnn = ["candle/cudnn", "candle-nn/cudnn"]
metal = ["candle/metal", "candle-nn/metal"]

[profile.release]
debug = true

[profile.release-no-debug]
inherits = "release"
debug = false

# The profile that 'dist' will build with
[profile.dist]
inherits = "release"
lto = "thin"
</file>

<file path="src/main.rs">
use anyhow::Result;
use clap::Parser;
use crossbeam_channel::unbounded;
use ears::{Model, TranscriptionOptions, audio};
use std::thread;

#[derive(Debug, Parser)]
struct Args {
    /// The audio input file, in wav/mp3/ogg/... format. If not provided, uses microphone.
    in_file: Option<String>,

    /// Use live microphone input instead of file.
    #[arg(long)]
    live: bool,

    /// List available audio devices.
    #[arg(long)]
    list_devices: bool,

    /// The repo where to get the model from.
    #[arg(long, default_value = "kyutai/stt-1b-en_fr-candle")]
    hf_repo: String,

    /// Run the model on cpu.
    #[arg(long)]
    cpu: bool,

    /// Display word level timestamps.
    #[arg(long)]
    timestamps: bool,

    /// Display the level of voice activity detection (VAD).
    #[arg(long)]
    vad: bool,

    /// Save the audio recording to a file (WAV format).
    #[arg(long)]
    save_audio: Option<String>,

    /// Select audio input device by index. Use --list-devices to see available devices.
    #[arg(long)]
    device: Option<usize>,

    /// Inject reference audio for language priming (esp, ger, jap)
    #[arg(long, short = 'l', value_parser = ["esp", "ger", "jap"])]
    lang: Option<String>,

    /// Start WebSocket server on specified port to stream transcription results
    #[arg(long)]
    ws: Option<u16>,

    /// Automatically terminate after no voice activity for specified seconds
    #[arg(long)]
    vad_timeout: Option<f64>,
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();

    if args.list_devices {
        return audio::list_audio_devices();
    }

    let options = TranscriptionOptions {
        timestamps: args.timestamps,
        vad: args.vad,
        save_audio: args.save_audio.clone(),
        vad_timeout: args.vad_timeout,
    };

    if args.live || args.in_file.is_none() {
        // Live microphone mode
        eprintln!("Loading model from repository: {}", args.hf_repo);
        let mut model = Model::load_from_hf(&args.hf_repo, args.cpu, options)?;

        if let Some(ref lang) = args.lang {
            let path = format!("ref_audio/{}.mp3", lang);
            if let Err(e) = model.prime_with_audio(&path) {
                eprintln!("Warning: failed to process reference audio {}: {}", path, e);
            }
        }

        let device_index = args.device;
        let save_audio_path = args.save_audio.as_deref();
        let ws_port = args.ws;
        
        let result = loop {
            let (audio_tx, audio_rx) = unbounded();

            // Start audio capture in a separate thread
            let _audio_handle = thread::spawn(move || {
                if let Err(e) = audio::start_audio_capture(audio_tx, device_index) {
                    eprintln!("Audio capture error: {}", e);
                }
            });

            let transcription_result = if let Some(ws_port) = ws_port {
                eprintln!("Starting WebSocket server on port {}", ws_port);
                eprintln!("Starting live transcription with WebSocket streaming. Press Ctrl+C to stop.");
                eprintln!("WebSocket endpoint: ws://localhost:{}/", ws_port);
                
                // Run live transcription with WebSocket streaming
                model.transcribe_live_ws(audio_rx, save_audio_path, ws_port).await
            } else {
                eprintln!("Starting live transcription. Press Ctrl+C to stop.");
                eprintln!("Transcription output:");
                eprintln!("{}", "-".repeat(50));

                // Run live transcription
                model.transcribe_live(audio_rx, save_audio_path)
            };

            match transcription_result {
                Ok(result) => break result,
                Err(e) => {
                    eprintln!("Transcription error: {}", e);
                    eprintln!("Attempting to restart audio capture...");
                    thread::sleep(std::time::Duration::from_secs(2));
                    continue;
                }
            }
        };

        if args.timestamps {
            for word in result.words {
                if let Some(end_time) = word.end_time {
                    println!("[{:5.2}-{:5.2}] {}", word.start_time, end_time, word.word);
                } else {
                    println!("[{:5.2}-     ] {}", word.start_time, word.word);
                }
            }
        } else {
            println!("{}", result.text);
        }

        // Audio handle cleanup is managed by the reconnection loop
    } else if let Some(ref in_file) = args.in_file {
        // File mode
        eprintln!("Loading audio file from: {}", in_file);
        eprintln!("Loading model from repository: {}", args.hf_repo);
        let mut model = Model::load_from_hf(&args.hf_repo, args.cpu, options)?;
        eprintln!("Running inference");

        let result = model.transcribe_file(in_file, args.save_audio.as_deref())?;

        if args.timestamps {
            for word in result.words {
                if let Some(end_time) = word.end_time {
                    println!("[{:5.2}-{:5.2}] {}", word.start_time, end_time, word.word);
                } else {
                    println!("[{:5.2}-     ] {}", word.start_time, word.word);
                }
            }
        } else {
            println!("{}", result.text);
        }
    } else {
        eprintln!("Either provide a file or use --live for microphone input");
        std::process::exit(1);
    }

    Ok(())
}
</file>

<file path="src/lib.rs">
use anyhow::Result;
use candle::{Device, Tensor};
use crossbeam_channel::Receiver;
use std::path::Path;

#[derive(Debug, serde::Deserialize)]
pub struct SttConfig {
    pub audio_silence_prefix_seconds: f64,
    pub audio_delay_seconds: f64,
}

#[derive(Debug, serde::Deserialize)]
pub struct Config {
    pub mimi_name: String,
    pub tokenizer_name: String,
    pub card: usize,
    pub text_card: usize,
    pub dim: usize,
    pub n_q: usize,
    pub context: usize,
    pub max_period: f64,
    pub num_heads: usize,
    pub num_layers: usize,
    pub causal: bool,
    pub stt_config: SttConfig,
}

impl Config {
    pub fn model_config(&self, vad: bool) -> moshi::lm::Config {
        let lm_cfg = moshi::transformer::Config {
            d_model: self.dim,
            num_heads: self.num_heads,
            num_layers: self.num_layers,
            dim_feedforward: self.dim * 4,
            causal: self.causal,
            norm_first: true,
            bias_ff: false,
            bias_attn: false,
            layer_scale: None,
            context: self.context,
            max_period: self.max_period as usize,
            use_conv_block: false,
            use_conv_bias: true,
            cross_attention: None,
            gating: Some(candle_nn::Activation::Silu),
            norm: moshi::NormType::RmsNorm,
            positional_embedding: moshi::transformer::PositionalEmbedding::Rope,
            conv_layout: false,
            conv_kernel_size: 3,
            kv_repeat: 1,
            max_seq_len: 4096 * 4,
            shared_cross_attn: false,
        };
        let extra_heads = if vad {
            Some(moshi::lm::ExtraHeadsConfig {
                num_heads: 4,
                dim: 6,
            })
        } else {
            None
        };
        moshi::lm::Config {
            transformer: lm_cfg,
            depformer: None,
            audio_vocab_size: self.card + 1,
            text_in_vocab_size: self.text_card + 1,
            text_out_vocab_size: self.text_card,
            audio_codebooks: self.n_q,
            conditioners: Default::default(),
            extra_heads,
        }
    }
}

pub struct Model {
    state: moshi::asr::State,
    text_tokenizer: sentencepiece::SentencePieceProcessor,
    timestamps: bool,
    vad: bool,
    config: Config,
    dev: Device,
    vad_timeout: Option<f64>,
}

#[derive(Debug, Clone)]
pub struct TranscriptionResult {
    pub text: String,
    pub words: Vec<WordTimestamp>,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct WordTimestamp {
    pub word: String,
    pub start_time: f64,
    pub end_time: Option<f64>,
}

/// WebSocket interface for real-time transcription streaming.
///
/// ## Connection
/// Connect to `ws://localhost:<port>/` where `<port>` is specified via the `--ws` option.
///
/// ## Message Format
/// All messages are JSON objects with a `type` field indicating the message type:
///
/// ### Word Message
/// Sent for each transcribed word as it's recognized:
/// ```json
/// {
///   "type": "word",
///   "word": "hello",
///   "start_time": 1.23,
///   "end_time": 1.45  // null for real-time words without end time yet
/// }
/// ```
///
/// ### Pause Message
/// Sent when voice activity detection detects a pause (requires --vad flag):
/// ```json
/// {
///   "type": "pause",
///   "timestamp": 1234567890.123
/// }
/// ```
///
/// ### Final Message
/// Sent at the end of transcription with complete results:
/// ```json
/// {
///   "type": "final",
///   "text": "complete transcribed text",
///   "words": [
///     {"word": "hello", "start_time": 1.23, "end_time": 1.45},
///     {"word": "world", "start_time": 1.46, "end_time": null}
///   ]
/// }
/// ```
///
/// ### Restart Command
/// Send from client to restart transcription after timeout or final message:
/// ```json
/// {
///   "type": "restart"
/// }
/// ```
///
/// ## Usage Example
/// ```bash
/// # Start transcription with WebSocket on port 8080
/// ears --live --ws 8080
///
/// # With timestamps and VAD
/// ears --live --ws 8080 --timestamps --vad
/// ```
#[derive(Debug, Clone, serde::Serialize)]
pub enum WebSocketMessage {
    Word {
        word: String,
        start_time: f64,
        end_time: Option<f64>,
    },
    Pause { timestamp: f64 },
    Final {
        text: String,
        words: Vec<WordTimestamp>,
    },
}

#[derive(Debug, Clone, serde::Deserialize)]
pub enum WebSocketCommand {
    Restart,
    Pause,
    Resume,
}

pub struct TranscriptionOptions {
    pub timestamps: bool,
    pub vad: bool,
    pub save_audio: Option<String>,
    pub vad_timeout: Option<f64>,
}

impl Default for TranscriptionOptions {
    fn default() -> Self {
        Self {
            timestamps: false,
            vad: false,
            save_audio: None,
            vad_timeout: None,
        }
    }
}

impl Model {
    pub fn load_from_hf(hf_repo: &str, cpu: bool, options: TranscriptionOptions) -> Result<Self> {
        let device = create_device(cpu)?;
        let dtype = device.bf16_default_to_f32();

        let api = hf_hub::api::sync::Api::new()?;
        let repo = api.model(hf_repo.to_string());
        let config_file = repo.get("config.json")?;
        let config: Config = serde_json::from_str(&std::fs::read_to_string(&config_file)?)?;
        let tokenizer_file = repo.get(&config.tokenizer_name)?;
        let model_file = repo.get("model.safetensors")?;
        let mimi_file = repo.get(&config.mimi_name)?;

        let text_tokenizer = sentencepiece::SentencePieceProcessor::open(&tokenizer_file)?;
        let vb_lm = unsafe {
            candle_nn::VarBuilder::from_mmaped_safetensors(&[&model_file], dtype, &device)?
        };
        let audio_tokenizer = moshi::mimi::load(mimi_file.to_str().unwrap(), Some(32), &device)?;
        let lm = moshi::lm::LmModel::new(
            &config.model_config(options.vad),
            moshi::nn::MaybeQuantizedVarBuilder::Real(vb_lm),
        )?;
        let asr_delay_in_tokens = (config.stt_config.audio_delay_seconds * 12.5) as usize;
        let state = moshi::asr::State::new(1, asr_delay_in_tokens, 0., audio_tokenizer, lm)?;

        Ok(Model {
            state,
            config,
            text_tokenizer,
            timestamps: options.timestamps,
            vad: options.vad,
            dev: device,
            vad_timeout: options.vad_timeout,
        })
    }

    pub fn prime_with_audio<P: AsRef<Path>>(&mut self, file_path: P) -> Result<()> {
        let (pcm, sample_rate) = kaudio::pcm_decode(file_path.as_ref())?;
        let pcm = if sample_rate != 24_000 {
            kaudio::resample(&pcm, sample_rate as usize, 24_000)?
        } else {
            pcm
        };

        for chunk in pcm.chunks(1920) {
            let tensor = Tensor::new(chunk, &self.dev)?.reshape((1, 1, chunk.len()))?;
            let _ = self
                .state
                .step_pcm(tensor, None, &().into(), |_, _, _| ())?;
        }
        Ok(())
    }

    pub fn transcribe_file<P: AsRef<Path>>(
        &mut self,
        file_path: P,
        save_audio: Option<&str>,
    ) -> Result<TranscriptionResult> {
        let (pcm, sample_rate) = kaudio::pcm_decode(file_path.as_ref())?;

        if let Some(save_path) = save_audio {
            self.save_audio_wav(&pcm, sample_rate, save_path)?;
        }

        let pcm = if sample_rate != 24_000 {
            kaudio::resample(&pcm, sample_rate as usize, 24_000)?
        } else {
            pcm
        };

        self.transcribe_pcm(pcm)
    }

    pub fn transcribe_live(
        &mut self,
        audio_rx: Receiver<Vec<f32>>,
        save_audio: Option<&str>,
    ) -> Result<TranscriptionResult> {
        use std::io::Write;
        use std::time::{Duration, Instant};

        let mut all_audio = Vec::new();
        let mut words = Vec::new();
        let mut current_text = String::new();
        let mut last_word: Option<(String, f64)> = None;
        let mut printed_eot = false;
        let mut last_voice_activity: Option<Instant> = None;

        loop {
            let pcm_chunk = match audio_rx.recv() {
                Ok(chunk) => chunk,
                Err(_) => {
                    eprintln!("Audio receiver channel closed");
                    return Err(anyhow::anyhow!("Audio receiver disconnected"));
                }
            };
            if save_audio.is_some() {
                all_audio.extend_from_slice(&pcm_chunk);
            }

            let mut has_voice_activity = false;

            for pcm in pcm_chunk.chunks(1920) {
                let pcm_tensor = Tensor::new(pcm, &self.dev)?.reshape((1, 1, ()))?;
                let asr_msgs = self
                    .state
                    .step_pcm(pcm_tensor, None, &().into(), |_, _, _| ())?;

                for asr_msg in asr_msgs.iter() {
                    match asr_msg {
                        moshi::asr::AsrMsg::Step { prs, .. } => {
                            if self.vad && prs[2][0] > 0.5 && !printed_eot {
                                printed_eot = true;
                                if !self.timestamps {
                                    print!(" <pause>");
                                    std::io::stdout().flush().ok();
                                }
                            }
                        }
                        moshi::asr::AsrMsg::EndWord { stop_time, .. } => {
                            printed_eot = false;
                            has_voice_activity = true;
                            if self.timestamps {
                                if let Some((word, start_time)) = last_word.take() {
                                    println!("[{start_time:5.2}-{stop_time:5.2}] {word}");
                                    words.push(WordTimestamp {
                                        word: word.clone(),
                                        start_time,
                                        end_time: Some(*stop_time),
                                    });
                                }
                            }
                        }
                        moshi::asr::AsrMsg::Word {
                            tokens, start_time, ..
                        } => {
                            printed_eot = false;
                            has_voice_activity = true;
                            let word = self
                                .text_tokenizer
                                .decode_piece_ids(tokens)
                                .unwrap_or_else(|_| String::new());

                            current_text.push(' ');
                            current_text.push_str(&word);

                            if !self.timestamps {
                                // Only show live transcription if we're in an interactive terminal
                                if atty::is(atty::Stream::Stdout) {
                                    print!(" {}", word);
                                    std::io::stdout().flush().ok();
                                }
                            } else {
                                if let Some((prev_word, prev_start_time)) = last_word.take() {
                                    println!(
                                        "[{prev_start_time:5.2}-{start_time:5.2}] {prev_word}"
                                    );
                                    words.push(WordTimestamp {
                                        word: prev_word,
                                        start_time: prev_start_time,
                                        end_time: Some(*start_time),
                                    });
                                }
                                last_word = Some((word, *start_time));
                            }
                        }
                    }
                }
            }

            // Update voice activity timestamp if we detected voice
            if has_voice_activity {
                last_voice_activity = Some(Instant::now());
            }

            // Check for timeout
            if let Some(timeout_secs) = self.vad_timeout {
                if let Some(last_activity) = last_voice_activity {
                    if last_activity.elapsed() > Duration::from_secs_f64(timeout_secs) {
                        break;
                    }
                }
            }
        }

        if let Some((word, start_time)) = last_word.take() {
            if self.timestamps {
                println!("[{start_time:5.2}-     ] {word}");
            }
            words.push(WordTimestamp {
                word,
                start_time,
                end_time: None,
            });
        }

        if !self.timestamps && atty::is(atty::Stream::Stdout) {
            println!();
        }

        if let Some(save_path) = save_audio {
            self.save_audio_wav(&all_audio, 24000, save_path)?;
        }

        Ok(TranscriptionResult {
            text: current_text.trim().to_string(),
            words,
        })
    }

    pub async fn transcribe_live_ws(
        &mut self,
        audio_rx: Receiver<Vec<f32>>,
        save_audio: Option<&str>,
        ws_port: u16,
    ) -> Result<TranscriptionResult> {
        use futures::{SinkExt, StreamExt};
        use std::io::Write;
        use std::sync::Arc;
        use tokio::sync::{broadcast, mpsc, watch};
        use tokio_tungstenite::{accept_async, tungstenite::Message};

        // WebSocket broadcast channel
        let (ws_tx, _ws_rx) = broadcast::channel(100);
        let ws_tx = Arc::new(ws_tx);

        // Channel used to request a restart of the transcription session
        let (restart_tx, mut restart_rx) = mpsc::unbounded_channel();
        let restart_tx = Arc::new(restart_tx);

        // Watch channel used to pause or resume transcription
        let (pause_tx, _pause_rx) = watch::channel(true);
        let pause_tx = Arc::new(pause_tx);

        // Spawn WebSocket server
        let listener = tokio::net::TcpListener::bind(format!("127.0.0.1:{}", ws_port)).await?;
        let ws_tx_clone = ws_tx.clone();
        let restart_tx_clone = restart_tx.clone();
        let pause_tx_clone = pause_tx.clone();
        tokio::spawn(async move {
            while let Ok((stream, _)) = listener.accept().await {
                let ws_tx = ws_tx_clone.clone();
                let restart_tx = restart_tx_clone.clone();
                let pause_tx = pause_tx_clone.clone();
                tokio::spawn(async move {
                    let ws_stream = match accept_async(stream).await {
                        Ok(ws) => ws,
                        Err(e) => {
                            eprintln!("WebSocket handshake error: {}", e);
                            return;
                        }
                    };

                    let (mut ws_sender, mut ws_receiver) = ws_stream.split();
                    let mut ws_rx = ws_tx.subscribe();

                    let receive_task = tokio::spawn(async move {
                        while let Some(msg) = ws_receiver.next().await {
                            match msg {
                                Ok(Message::Close(_)) => break,
                                Ok(Message::Text(text)) => {
                                    if let Ok(cmd) = serde_json::from_str::<WebSocketCommand>(&text)
                                    {
                                        match cmd {
                                            WebSocketCommand::Restart => {
                                                let _ = restart_tx.send(());
                                            }
                                            WebSocketCommand::Pause => {
                                                let _ = pause_tx.send(true);
                                            }
                                            WebSocketCommand::Resume => {
                                                let _ = pause_tx.send(false);
                                            }
                                        }
                                    }
                                }
                                Err(e) => {
                                    eprintln!("WebSocket receive error: {}", e);
                                    break;
                                }
                                _ => {}
                            }
                        }
                    });

                    let send_task = tokio::spawn(async move {
                        while let Ok(ws_msg) = ws_rx.recv().await {
                            let json_msg = serde_json::to_string(&ws_msg).unwrap_or_default();
                            if ws_sender.send(Message::Text(json_msg)).await.is_err() {
                                break;
                            }
                        }
                    });

                    let _ = tokio::join!(receive_task, send_task);
                });
            }
        });

        // Bridge blocking audio receiver to async channel
        let (pcm_tx, mut pcm_rx) = mpsc::unbounded_channel();
        let mut pause_rx = pause_tx.subscribe();
        std::thread::spawn(move || {
            while let Ok(chunk) = audio_rx.recv() {
                if pcm_tx.send(chunk).is_err() {
                    break;
                }
            }
        });

        let mut all_audio = Vec::new();
        let mut overall_words = Vec::new();
        let mut overall_text = String::new();

        loop {
            let mut words = Vec::new();
            let mut current_text = String::new();
            let mut last_word: Option<(String, f64)> = None;
            let mut printed_eot = false;
            let mut last_voice_activity: Option<std::time::Instant> = None;
            let mut restart = false;
            let mut paused = *pause_rx.borrow();

            eprintln!("Starting transcription session (paused - send Resume command to begin)...");

            loop {
                tokio::select! {
                    _ = restart_rx.recv() => {
                        eprintln!("Received restart command");
                        restart = true;
                        break;
                    }
                    _ = pause_rx.changed() => {
                        paused = *pause_rx.borrow();
                        if paused {
                            eprintln!("Transcription paused");
                        } else {
                            eprintln!("Transcription resumed");
                        }
                    }
                    Some(pcm_chunk) = pcm_rx.recv() => {
                        if paused {
                            continue;
                        }
                        if save_audio.is_some() {
                            all_audio.extend_from_slice(&pcm_chunk);
                        }

                        let mut has_voice_activity = false;

                        for pcm in pcm_chunk.chunks(1920) {
                            let pcm_tensor = Tensor::new(pcm, &self.dev)?.reshape((1, 1, ()))?;
                            let asr_msgs = self.state.step_pcm(pcm_tensor, None, &().into(), |_, _, _| ())?;

                            for asr_msg in asr_msgs.iter() {
                                match asr_msg {
                                    moshi::asr::AsrMsg::Step { prs, .. } => {
                                        if self.vad && prs[2][0] > 0.5 && !printed_eot {
                                            printed_eot = true;
                                            let pause_msg = WebSocketMessage::Pause {
                                                timestamp: std::time::SystemTime::now()
                                                    .duration_since(std::time::UNIX_EPOCH)
                                                    .unwrap_or_default()
                                                    .as_secs_f64(),
                                            };
                                            let _ = ws_tx.send(pause_msg);

                                            if !self.timestamps {
                                                print!(" <pause>");
                                                std::io::stdout().flush().ok();
                                            }
                                        }
                                    }
                                    moshi::asr::AsrMsg::EndWord { stop_time, .. } => {
                                        printed_eot = false;
                                        has_voice_activity = true;
                                        if self.timestamps {
                                            if let Some((word, start_time)) = last_word.take() {
                                                println!("[{start_time:5.2}-{stop_time:5.2}] {word}");
                                                let word_ts = WordTimestamp {
                                                    word: word.clone(),
                                                    start_time,
                                                    end_time: Some(*stop_time),
                                                };
                                                words.push(word_ts.clone());
                                                let ws_msg = WebSocketMessage::Word {
                                                    word: word_ts.word,
                                                    start_time: word_ts.start_time,
                                                    end_time: word_ts.end_time,
                                                };
                                                let _ = ws_tx.send(ws_msg);
                                            }
                                        }
                                    }
                                    moshi::asr::AsrMsg::Word { tokens, start_time, .. } => {
                                        printed_eot = false;
                                        has_voice_activity = true;
                                        let word = self.text_tokenizer
                                            .decode_piece_ids(tokens)
                                            .unwrap_or_else(|_| String::new());

                                        current_text.push(' ');
                                        current_text.push_str(&word);

                                        if !self.timestamps {
                                            print!(" {}", word);
                                            std::io::stdout().flush().ok();

                                            let ws_msg = WebSocketMessage::Word {
                                                word: word.clone(),
                                                start_time: *start_time,
                                                end_time: None,
                                            };
                                            let _ = ws_tx.send(ws_msg);
                                        } else {
                                            if let Some((prev_word, prev_start_time)) = last_word.take() {
                                                println!("[{prev_start_time:5.2}-{start_time:5.2}] {prev_word}");
                                                let word_ts = WordTimestamp {
                                                    word: prev_word.clone(),
                                                    start_time: prev_start_time,
                                                    end_time: Some(*start_time),
                                                };
                                                words.push(word_ts.clone());
                                                let ws_msg = WebSocketMessage::Word {
                                                    word: word_ts.word,
                                                    start_time: word_ts.start_time,
                                                    end_time: word_ts.end_time,
                                                };
                                                let _ = ws_tx.send(ws_msg);
                                            }
                                            last_word = Some((word, *start_time));
                                        }
                                    }
                                }
                            }
                        }

                        if has_voice_activity {
                            last_voice_activity = Some(std::time::Instant::now());
                        }

                        if let Some(timeout_secs) = self.vad_timeout {
                            if let Some(last_activity) = last_voice_activity {
                                if last_activity.elapsed() > std::time::Duration::from_secs_f64(timeout_secs) {
                                    eprintln!("Voice activity timeout reached");
                                    break;
                                }
                            }
                        }
                    }
                }
            }

            if let Some((word, start_time)) = last_word.take() {
                if self.timestamps {
                    println!("[{start_time:5.2}-     ] {word}");
                }
                let word_ts = WordTimestamp {
                    word: word.clone(),
                    start_time,
                    end_time: None,
                };
                words.push(word_ts.clone());
                let ws_msg = WebSocketMessage::Word {
                    word: word_ts.word,
                    start_time: word_ts.start_time,
                    end_time: word_ts.end_time,
                };
                let _ = ws_tx.send(ws_msg);
            }

            if !self.timestamps {
                println!();
            }

            overall_words.extend(words.clone());
            if !current_text.is_empty() {
                if !overall_text.is_empty() {
                    overall_text.push(' ');
                }
                overall_text.push_str(current_text.trim());
            }

            let session_result = TranscriptionResult {
                text: current_text.trim().to_string(),
                words: words.clone(),
            };

            let final_msg = WebSocketMessage::Final {
                text: session_result.text.clone(),
                words: session_result.words.clone(),
            };
            let _ = ws_tx.send(final_msg);

            if !restart {
                break;
            }
        }

        if let Some(save_path) = save_audio {
            self.save_audio_wav(&all_audio, 24000, save_path)?;
        }

        Ok(TranscriptionResult {
            text: overall_text,
            words: overall_words,
        })
    }
    fn transcribe_pcm(&mut self, mut pcm: Vec<f32>) -> Result<TranscriptionResult> {
        if self.config.stt_config.audio_silence_prefix_seconds > 0.0 {
            let silence_len =
                (self.config.stt_config.audio_silence_prefix_seconds * 24000.0) as usize;
            pcm.splice(0..0, vec![0.0; silence_len]);
        }

        let suffix = (self.config.stt_config.audio_delay_seconds * 24000.0) as usize;
        pcm.resize(pcm.len() + suffix + 24000, 0.0);

        let mut words = Vec::new();
        let mut current_text = String::new();
        let mut last_word: Option<(String, f64)> = None;

        for pcm_chunk in pcm.chunks(1920) {
            let pcm_tensor = Tensor::new(pcm_chunk, &self.dev)?.reshape((1, 1, ()))?;
            let asr_msgs = self
                .state
                .step_pcm(pcm_tensor, None, &().into(), |_, _, _| ())?;

            for asr_msg in asr_msgs.iter() {
                match asr_msg {
                    moshi::asr::AsrMsg::Step { .. } => {
                        // Handle step messages if needed for VAD or other processing
                    }
                    moshi::asr::AsrMsg::EndWord { stop_time, .. } => {
                        if self.timestamps {
                            if let Some((word, start_time)) = last_word.take() {
                                words.push(WordTimestamp {
                                    word: word.clone(),
                                    start_time,
                                    end_time: Some(*stop_time),
                                });
                            }
                        }
                    }
                    moshi::asr::AsrMsg::Word {
                        tokens, start_time, ..
                    } => {
                        let word = self
                            .text_tokenizer
                            .decode_piece_ids(tokens)
                            .unwrap_or_else(|_| String::new());

                        current_text.push(' ');
                        current_text.push_str(&word);

                        if self.timestamps {
                            if let Some((prev_word, prev_start_time)) = last_word.take() {
                                words.push(WordTimestamp {
                                    word: prev_word,
                                    start_time: prev_start_time,
                                    end_time: Some(*start_time),
                                });
                            }
                            last_word = Some((word, *start_time));
                        }
                    }
                }
            }
        }

        if let Some((word, start_time)) = last_word.take() {
            words.push(WordTimestamp {
                word,
                start_time,
                end_time: None,
            });
        }

        Ok(TranscriptionResult {
            text: current_text.trim().to_string(),
            words,
        })
    }

    fn save_audio_wav(&self, pcm: &[f32], sample_rate: u32, path: &str) -> Result<()> {
        use std::fs::File;
        use std::io::{BufWriter, Write};

        let mut file = BufWriter::new(File::create(path)?);

        // WAV header
        let data_size = (pcm.len() * 2) as u32; // 16-bit samples
        let file_size = data_size + 36;

        // RIFF header
        file.write_all(b"RIFF")?;
        file.write_all(&file_size.to_le_bytes())?;
        file.write_all(b"WAVE")?;

        // fmt chunk
        file.write_all(b"fmt ")?;
        file.write_all(&16u32.to_le_bytes())?; // chunk size
        file.write_all(&1u16.to_le_bytes())?; // audio format (PCM)
        file.write_all(&1u16.to_le_bytes())?; // num channels
        file.write_all(&sample_rate.to_le_bytes())?; // sample rate
        file.write_all(&(sample_rate * 2).to_le_bytes())?; // byte rate
        file.write_all(&2u16.to_le_bytes())?; // block align
        file.write_all(&16u16.to_le_bytes())?; // bits per sample

        // data chunk
        file.write_all(b"data")?;
        file.write_all(&data_size.to_le_bytes())?;

        // audio data (convert f32 to i16)
        for &sample in pcm {
            let sample_i16 = (sample.clamp(-1.0, 1.0) * 32767.0) as i16;
            file.write_all(&sample_i16.to_le_bytes())?;
        }

        file.flush()?;
        Ok(())
    }
}

pub fn create_device(cpu: bool) -> Result<Device> {
    if cpu {
        Ok(Device::Cpu)
    } else if candle::utils::cuda_is_available() {
        Ok(Device::new_cuda(0)?)
    } else if candle::utils::metal_is_available() {
        Ok(Device::new_metal(0)?)
    } else {
        Ok(Device::Cpu)
    }
}

pub mod audio {
    use anyhow::Result;
    use cpal::traits::{DeviceTrait, HostTrait, StreamTrait};
    use crossbeam_channel::Sender;
    use std::thread;

    pub fn list_audio_devices() -> Result<()> {
        let host = cpal::default_host();

        println!("Available input devices:");
        for (i, device) in host.input_devices()?.enumerate() {
            let name = device.name().unwrap_or("Unknown".to_string());
            println!("  {}: {}", i, name);
        }

        Ok(())
    }

    pub fn start_audio_capture(
        audio_tx: Sender<Vec<f32>>,
        device_index: Option<usize>,
    ) -> Result<()> {
        let max_retries = 3;
        let mut retry_count = 0;

        loop {
            match start_audio_capture_internal(audio_tx.clone(), device_index) {
                Ok(()) => break,
                Err(e) => {
                    retry_count += 1;
                    if retry_count >= max_retries {
                        return Err(e);
                    }
                    eprintln!(
                        "Audio capture failed (attempt {}/{}): {}",
                        retry_count, max_retries, e
                    );
                    eprintln!("Retrying audio capture in 1 second...");
                    thread::sleep(std::time::Duration::from_secs(1));
                }
            }
        }
        Ok(())
    }

    fn start_audio_capture_internal(
        audio_tx: Sender<Vec<f32>>,
        device_index: Option<usize>,
    ) -> Result<()> {
        let host = cpal::default_host();
        let device = if let Some(index) = device_index {
            host.input_devices()?
                .nth(index)
                .ok_or_else(|| anyhow::anyhow!("Device index {} not found", index))?
        } else {
            host.default_input_device()
                .ok_or_else(|| anyhow::anyhow!("No input device available"))?
        };

        let config = device.default_input_config()?;
        let sample_rate = config.sample_rate().0;

        eprintln!(
            "Using input device: {}",
            device.name().unwrap_or("Unknown".to_string())
        );
        eprintln!("Sample rate: {}", sample_rate);

        let audio_tx_clone = audio_tx.clone();
        let stream = match config.sample_format() {
            cpal::SampleFormat::F32 => {
                let config = config.into();
                device.build_input_stream(
                    &config,
                    move |data: &[f32], _: &cpal::InputCallbackInfo| {
                        let resampled = if sample_rate != 24000 {
                            let ratio = 24000.0 / sample_rate as f32;
                            let new_len = (data.len() as f32 * ratio) as usize;
                            let mut resampled = Vec::with_capacity(new_len);

                            for i in 0..new_len {
                                let pos = i as f32 / ratio;
                                let idx = pos as usize;
                                let frac = pos - idx as f32;

                                if idx + 1 < data.len() {
                                    let sample = data[idx] * (1.0 - frac) + data[idx + 1] * frac;
                                    resampled.push(sample);
                                } else if idx < data.len() {
                                    resampled.push(data[idx]);
                                }
                            }
                            resampled
                        } else {
                            data.to_vec()
                        };

                        if audio_tx_clone.send(resampled).is_err() {
                            eprintln!("Audio receiver disconnected");
                            return;
                        }
                    },
                    |err| {
                        eprintln!("Audio stream error: {}", err);
                        std::process::exit(1);
                    },
                    None,
                )?
            }
            _ => {
                return Err(anyhow::anyhow!(
                    "Unsupported sample format. Only F32 is supported."
                ));
            }
        };

        stream.play()?;

        loop {
            thread::sleep(std::time::Duration::from_millis(100));
        }
    }
}
</file>

</files>

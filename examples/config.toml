# eaRS Example Configuration
#
# Copy this file to one of these locations on first run:
# - $XDG_CONFIG_HOME/ears/config.toml
# - ~/.config/ears/config.toml
#
# Notes
# - "~" is expanded to your home directory.
# - Use "default" for locations that should use built-in defaults (e.g., HuggingFace cache).
# - Paths can be absolute; directories are created on first run when needed.

[storage]
# Model storage directory
# - "default": use HuggingFace's default cache directory
# - custom path: store/download models under this directory
#   (e.g., "~/Models/ears")
model_dir = "default"

# Reference audio directory for language priming
# On first run, required reference files (esp, ger, jap, ita, por) are installed here.
ref_audio = "~/.local/share/ears/ref_audio"

[whisper]
# Enable Whisper enhancement for higher-accuracy corrections
# Requires the "whisper" feature/build to be available
enabled = false

# Default Whisper model to use (examples: "large-v3-turbo", "large-v3", "medium")
default_model = "large-v3-turbo"

# Model format: "gguf" (whisper.cpp) or "safetensors"
model_format = "gguf"

# Quantization level for GGUF models: "Q5_0", "Q8_0", or "f32"
quantization = "Q5_0"

# Languages to enhance (ISO 639-1 codes). Example set shown.
# This list is used to indicate which languages benefit from Whisper correction.
languages = ["de", "ja", "es", "it"]

# Minimum confidence required to accept Whisper corrections (0.0 – 1.0)
confidence_threshold = 0.7

# Storage directory for Whisper models
# - "default": use $XDG_CACHE_HOME/huggingface/whisper-models (or ~/.cache/...)
# - custom path: where GGUF/Safetensors model files are stored
storage_dir = "default"

[whisper.sentence_detection]
# Minimum/maximum sentence duration (seconds)
min_duration = 1.0
max_duration = 30.0

# Voice Activity Detection (VAD) confidence threshold for pauses
vad_pause_threshold = 0.8

# Minimum silence to consider a sentence boundary (seconds)
silence_duration = 0.5

# Punctuation markers treated as sentence endings
punctuation_markers = [".", "!", "?", "。", "！", "？"]

[server]
# WebSocket server port for streaming transcription results
# Endpoint: ws://localhost:8765/
websocket_port = 8765

[hotkeys]
# Enable internal global hotkeys (disable if your WM binds keys itself)
enable_internal = true

# Toggle live dictation (start/stop)
toggle = "ctrl+shift+v"

# Cycle language for priming / WS SetLanguage
language_cycle = "ctrl+shift+l"
